# Simple Linear Regression

[相关性](correlation.html)是简单线性回归它爹，回归的概念来自于Francis Galton，他发现高个子男人生出来的儿子会比自己矮，而矮个子男人生出来的儿子会比自己高，并称这种现象为[回归平庸(regression toward mediocrity)](http://www.biostat.washington.edu/~bsweir/BIOST551/Galton1886.pdf)，现在被称之为回归均值(regression to the mean)，Galton有个学生叫Karl Pearson，他给出了相关性和回归的数学公式，这也是相关系数被命名为Pearson相关系数的原因，而相关系数的符号r则取自于回归(regression)一词。

简单线性回归名副其实，非常简单，在假定X和Y是线性关系，使用单变量X来预测Y。

$$ Y \approx \beta_0 + \beta_1 X $$

假设我们要用花萼长度来预测花瓣长度，
```{r lm.fig1, fig.width=6, fig.height=6}
require(ggplot2)
data(iris)
attach(iris)
p <- ggplot(iris, aes(Sepal.Length, Petal.Length))+
		geom_point(shape=1, color="red")
print(p)
```
我们需要拟合：
$$ Petal \approx \beta_0 + \beta_1 Sepal $$

$\beta_0$ 和 $\beta_1$ 代表线性模型的截距和斜率，这两个模型参数是未知的，需要通过训练数据估计 $\hat{\beta_0}$ 和 $\hat{\beta_1}$ 。

使得直线
$$ \hat{y} = \hat{\beta_0} + \hat{\beta_1} x $$
与数据集中的点最接近，有多种方法来来计算"接近度“，最常用的是最小二乘法(least squares)。

最小二乘法通过计算残差平方和（RSS, residual sum of squares）
	$$ RSS = {e_1}^2 + {e_2}^2 + … + {e_n}^2 $$
其中 $e_i = y_i - \hat{y_i}$ .
问题转换为找出 $\hat{\beta_0}$ 和 $\hat{\beta_1}$ 使得RSS的值最小。
通过计算，可以得到：
$$ \hat{\beta_1} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} = r_{xy} (\frac{s_y}{s_x})$$
$$ \hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x} $$

```{r}
b1 <- sum((Petal.Length-mean(Petal.Length)) * (Sepal.Length-mean(Sepal.Length)))/sum((Sepal.Length-mean(Sepal.Length))^2)
b0 <- mean(Petal.Length) - b1 * mean(Sepal.Length)
cat("Intersect:\t", b0, "\n", "Slope:\t", b1, "\n")
```

stats包中的lm()函数，用于拟合线性模型。
```{r}
model <- lm(Petal.Length ~ Sepal.Length, data=iris)
model
```

## 评估参数准确性
打个比方，我们使用样本均值 $\hat{\mu}$ 来估计总体均值 $\mu$ ，对于任意一个样本的 $\hat{\mu}$ 值，有可能会高估 $\mu$ ，也有可能会低估，需要量化到底 $\hat{\mu}$ 偏离 $\mu$ 有多远，这个问题通过计算standard error of $\hat{\mu}$ 来解决。

同样地，我们估计出来的参数 $\hat{\beta_0}$ 和 $\hat{\beta_1}$ 和真实的参数到底偏离多远，需要通过计算参数的standard error来估计。
$$ SE(\hat{\beta_0})^2 = \sigma^2 [\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i-\bar{x})^2}] $$
$$ SE(\hat{\beta_1})^2 = \frac{\sigma^2}{\sum_{i=1}^n (x_i-\bar{x})^2} $$

其中 $\sigma^2 = Var(\epsilon)$ ，通常情况下是未知的，使用残差标准误 $RSE=\sqrt{RSS/(n-2)}$ 来估计。


```{r lm.fig2, fig.width=6, fig.height=6}
p+geom_smooth(method="lm", se=TRUE, level=0.95)
```
上图中，阴影部分就是参数的95%置信区间。

有了标准误，还可以用统计检验来检测X和Y是否具有相关性。
在这里，可以使用t检验，计算t统计量：
$$ t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})} $$

这些统计量，lm()函数都会计算。
```{r}
summary(model)
```

## 评估模型准确性
### 残差标准误(RSE, residual standard error)
```{r lm.fig3, fig.width=6, fig.height=6}
iris$fitted <- predict(model)
p %+% iris +  aes(x=fitted, y=Petal.Length-fitted) + geom_linerange(aes(ymin = 0, ymax = Petal.Length - fitted), colour = "purple") + geom_hline(aes(yintercept = 0)) + ggtitle("Residual Distribution")+ylab("Residual")
```

RSE是Y值和回归直线偏离值均值：
$$ RSE = \sqrt{\frac{RSS}{n-2}} = \sqrt{\frac{\sum_{i=1}^n (y_i-\hat{y_i})^2}{n-2}} $$

```{r}
rse <- with(iris, sqrt(sum((fitted - Petal.Length)^2)/(length(Petal.Length)-2)))
print(rse)
```
通过RSE，可以计算模型预测值和真实值平均水平偏离多少。偏离量大不大，可以用 $RSE/\bar{y}$ 来估计。
```{r}
with(iris, rse/mean(Petal.Length))
```
RSE度量的是失拟（lack of fit），如果RSE很小，则 $\hat{y_i}$ 和 $y_i$ 很接近，模型对数据的拟合非常好，如果RSE很大，则表明模型对数据的拟合很差。

### $R^2$ 统计量
RSE是绝对值，不够清晰，用 $RSE/\bar{y}$ 相对值会好一些。 $R^2$ 提供另外一种度量方式:
$$ R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}$$

其中 $TSS=\sum(y_i - \bar{y})^2$ ,TSS度量Y的方差，也就是拟合前总的方差；而RSS度量的是残差的方差，也就是拟合后无法解释的方差；TSS-RSS度量的是能够由拟合模型解释的方差；继而， $R^2$ 统计量度量的是Y的方差能由X来解释的比例。

 $R^2$ 接近1，表明回归能解释Y的方差，回归模型拟合得好。而接近0的话，则表明无法解释Y的大部分方差，拟合模型很差，甚至可能是错的。
 
```{r}
tss <- with(iris, sum((Petal.Length - mean(Petal.Length))^2))
rss <- with(iris, sum((fitted-Petal.Length)^2))
rr <- 1 - rss/tss
rr
```
 $R^2$ 统计量度量的是X和Y的线性相关性，我们知道相关系数r定义为：
 $$ Cor(X, Y) = \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2)}\sqrt{\sum_{i=1}^n (y_i - \bar{y})^2)}}$$

同样我们可以用相关系数r来评估模型，事实上 $R^2 = r^2$ ，可以说 $R^2$ 是 $r^2$ 的通用形式，相关系数只能用于单变量，如果要用多变量做线性回归的话，就没法用，从这个角度来看，也可以说 $R^2$ 是 $r^2$ 的扩展形式。

## 方差分析
如前所述，Y的方差TSS，由两部分组成，残差方差(RSS)和回归方差(TSS-RSS)，继而我们可以进行方差分析，TSS的自由度是n-1, 回归方差是1(简单线性回归是单变量),残差方差的自由度是n-2 （TSS的df - 回归方差df），将方差除以自由度，得到平均方差。

如果不存在线性关系，那么回归平均方差和残差平均方差大致相等。可以使用F统计量来检验是否存在线性关系。
$$ F = \frac{regression\; mean\; square}{residual\; mean\; square} = \frac{TSS-RSS}{RSS/(n-2)}$$

F统计量服从1和n-2自由度的F分析，继而可以计算出p值，当然可以直接扔给anova函数，进行统计计算。
```{r}
n <- nrow(iris)
fstat <- (tss-rss)/(rss/(n-2))
print(fstat)
pf(fstat, 1, n-2, lower.tail=F)
anova(model)
```

## 可视化辅助诊断模型
评估模型最重要的指标是残差，R提供了函数可视化残差，残差 vs 拟合值, 残差开方 vs 拟合值, 残差的QQ图，标准化残差 vs Leverage, [leverage度量的是数据点对回归线的影响](http://onlinestatbook.com/2/regression/influential.html)。

```{r lm.fig4, fig.width=6, fig.height=6}
par(mfrow=c(2,2))
plot(model)
```


```
2013-06-18
```

## 欢迎请作者一杯咖啡
<p><a href="https://me.alipay.com/gcyu"><img src="https://img.alipay.com/sys/personalprod/style/mc/btn-index.png" alt="捐赠EventProxy"></a></p>

## 访问统计
  <a href="http://www.digits.com" target="_blank">
    <img src="http://counter.digits.com/?counter={1bb4e1b1-f35f-dd34-31dc-5a118f10bdb1}&template=simplehits&foreground=0B77CF&background=FFFFFF" 
     alt="Hit Counter by Digits" border="0"  />
  </a>
